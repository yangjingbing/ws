project.cluster=ssp
project.name=api
env=test
monitor.enable=true
tracing.enable=true
tracing.sampleRate=1.0

es-search.host=test-es.hupofintech.com:443

db.host=localhost:3306
db=csjj_pay

### Database for MySQL ###
spring.datasource.url=jdbc:mysql://${db.host}/${db}?serverTimezone=UTC&useTimezone=true&useUnicode=true&characterEncoding=UTF-8\
  &zeroDateTimeBehavior=convertToNull&statementInterceptors=brave.mysql.TracingStatementInterceptor&allowMultiQueries=true&useSSL=false
spring.datasource.username=root
spring.datasource.password=root
spring.datasource.driver-class-name=com.mysql.jdbc.Driver

### Server ###
server.port=39898
server.servlet.contextPath=/api

### MyBatis ###
mybatis.mapper-locations=classpath*:com/hupo/service/spring/common/*/*Mapper.java
mybatis.type-aliases-package=com.hupo.service.spring.common.cache

### Logging ###
# Log levels (TRACE, DEBUG, INFO, WARN, ERROR, FATAL, OFF)
logging.level.root=INFO
logging.level.org.springframework=INFO
logging.level.org.springframework.web=INFO
logging.level.org.springframework.transaction=DEBUG
logging.level.org.mybatis=INFO
logging.level.com.cp=DEBUG
# File output
logging.file=/logs/${project.name}.log

hupo.management.appId=piggy-bank
hupo.management.secret=piggy-bank-adf121332

spring.servlet.multipart.max-file-size=3MB
spring.servlet.multipart.max-request-size=10MB
spring.http.multipart.enabled=false

swagger.enable=true

### Druid ###
# 配置druid，所有spring配置参考spring-configuration-metadata.json，全局搜索即可。
# 另外注释可以参考https://github.com/drtrang/druid-spring-boot/blob/master/druid-spring-boot-example/druid-spring-boot-mybatis-example/src/main/resources/druid.yml

### format sql
spring.datasource.druid.filter.statement-sql-pretty-format=true
### {conn-410001, pstmt-420002} created. $sql
spring.datasource.druid.filter.slf4j.statement-create-after-log-enabled=false
### {conn-410001, pstmt-420002} Parameters :[]
### {conn-410001, pstmt-420002} Types :[]
spring.datasource.druid.filter.slf4j.statement-parameter-set-log-enabled=false

spring.jpa.properties.hibernate.jdbc.batch_size=150